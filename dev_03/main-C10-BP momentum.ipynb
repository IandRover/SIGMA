{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffa0c9f-8276-4e73-b6a8-bf33d01039d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"dcf9600e0485401cbb0ddbb0f7be1c70f96b32ef\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "import wandb\n",
    "from sigma_layer import SigmaLinear, SigmaConv, SigmaView\n",
    "from utils import get_dataset, gradient_centralization, normalize_along_axis, get_activation_function, compute_SCL_loss\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f367862f-05b5-4c98-b3be-7a36a686df88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: CIFAR10_BP_conv32-actelu_SGD_0.01_2023\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "No LN1\n",
      "Epoch 0 | train loss 1.6191 | val loss 1.2043 | val acc 59.2300\n",
      "Epoch 1 | train loss 1.1048 | val loss 0.9602 | val acc 66.4500\n",
      "Epoch 2 | train loss 0.9234 | val loss 0.8447 | val acc 70.6200\n",
      "Epoch 3 | train loss 0.8068 | val loss 0.7595 | val acc 73.2300\n",
      "Epoch 4 | train loss 0.7055 | val loss 0.6948 | val acc 75.1300\n",
      "Epoch 5 | train loss 0.6188 | val loss 0.6597 | val acc 76.2100\n",
      "Epoch 6 | train loss 0.5400 | val loss 0.6362 | val acc 77.5400\n",
      "Epoch 7 | train loss 0.4745 | val loss 0.6129 | val acc 78.1300\n",
      "Epoch 8 | train loss 0.4174 | val loss 0.6278 | val acc 78.6900\n",
      "Epoch 9 | train loss 0.3566 | val loss 0.6212 | val acc 79.2500\n",
      "Epoch 10 | train loss 0.3002 | val loss 0.6311 | val acc 78.9500\n",
      "Epoch 11 | train loss 0.2482 | val loss 0.6546 | val acc 79.3300\n",
      "Epoch 12 | train loss 0.2099 | val loss 0.6657 | val acc 79.6000\n",
      "Epoch 13 | train loss 0.1618 | val loss 0.7167 | val acc 79.5500\n",
      "Epoch 14 | train loss 0.1216 | val loss 0.7338 | val acc 79.7000\n",
      "Epoch 15 | train loss 0.0973 | val loss 0.7733 | val acc 79.3000\n",
      "Epoch 16 | train loss 0.0620 | val loss 0.8584 | val acc 79.5900\n",
      "Epoch 17 | train loss 0.0488 | val loss 0.8464 | val acc 79.3100\n",
      "Epoch 18 | train loss 0.0228 | val loss 0.9007 | val acc 79.2100\n",
      "Epoch 19 | train loss 0.0149 | val loss 0.9931 | val acc 78.8800\n",
      "Epoch 20 | train loss -0.0001 | val loss 0.9969 | val acc 78.9500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 173\u001b[0m\n\u001b[1;32m    170\u001b[0m     _, loss_item \u001b[39m=\u001b[39m criteria(activations, signals, target\u001b[39m.\u001b[39mto(device), method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 173\u001b[0m     activations \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mto(device), detach_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    174\u001b[0m     _, loss_item \u001b[39m=\u001b[39m criteria(activations, signals, target\u001b[39m.\u001b[39mto(device), method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    175\u001b[0m prediction \u001b[39m=\u001b[39m activations[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 86\u001b[0m, in \u001b[0;36mSigmaModel_SimpleCNN.forward\u001b[0;34m(self, x, detach_grad, return_activations)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, detach_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_activations\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):   \n\u001b[1;32m     84\u001b[0m     \u001b[39m# x = self.LN1(x)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     a1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x, detach_grad)\n\u001b[0;32m---> 86\u001b[0m     a2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(a1, detach_grad)\n\u001b[1;32m     87\u001b[0m     a2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mview1(a2, detach_grad)\n\u001b[1;32m     88\u001b[0m     a3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(a2, detach_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/SIGMA/sigma_layer.py:61\u001b[0m, in \u001b[0;36mSigmaConv.forward\u001b[0;34m(self, x, detach_grad)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m detach_grad: x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     60\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_act(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_layer(x))\n\u001b[0;32m---> 61\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(x)\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooked_features \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, ceil_mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_indices)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmax_pool2d(\u001b[39minput\u001b[39m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "# Training scheme group\n",
    "method_parser = parser.add_argument_group(\"Method\")\n",
    "method_parser.add_argument('--method', type=str, default='BP', choices=['SIGMA', 'BP', 'FA'])\n",
    "method_parser.add_argument('--architecture', type=str, default='lenet', choices=['lenet', 'vgg'])\n",
    "method_parser.add_argument('--actfunc', type=str, default='elu', choices=['tanh', 'elu', 'relu'])\n",
    "method_parser.add_argument('--conv_dim', type=int, default=32, choices=[32, 64])\n",
    "# Dataset group\n",
    "dataset_parser = parser.add_argument_group('Dataset')\n",
    "dataset_parser.add_argument('--dataset', type=str, default='CIFAR10', choices=['MNIST', 'CIFAR10'])\n",
    "dataset_parser.add_argument('--batchsize', type=int, default=128)\n",
    "dataset_parser.add_argument('--splitratio', type=float, default=0.2)\n",
    "# Training group # LR, optimizer, weight_decay, momentum\n",
    "training_parser = parser.add_argument_group('Training')\n",
    "training_parser.add_argument('--epochs', type=int, default=100)\n",
    "training_parser.add_argument('--lr', type=float, default=0.01)\n",
    "training_parser.add_argument('--optimizer', type=str, default='SGD', choices=['RMSprop', 'Adam', 'SGD'])\n",
    "# Seed group\n",
    "seed_parser = parser.add_argument_group('Seed')\n",
    "seed_parser.add_argument('--seed', type=int, default=2023)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Set run_name\n",
    "run_name = f\"{args.dataset}_{args.method}_conv{args.conv_dim}-act{args.actfunc}_{args.optimizer}_{args.lr}_{args.seed}\"\n",
    "time_stamp = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Set wandb\n",
    "wandb.init(\n",
    "    project=\"opt-sigma\",\n",
    "    name=run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"algorithm\": args.method,\n",
    "    \"architecture\": \"SimpleCNN\",\n",
    "    \"dataset\": args.dataset,\n",
    "    \"epochs\": args.epochs,\n",
    "    \"lr\": args.lr,\n",
    "    \"optimizer\": args.optimizer,\n",
    "    \"seed\": args.seed,\n",
    "    \"conv_dim\": args.conv_dim,\n",
    "    \"actfunc\": args.actfunc,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(args.seed), np.random.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n",
    "\n",
    "# Get dataset\n",
    "train_loader, val_loader, test_loader = get_dataset(args)\n",
    "\n",
    "class SigmaModel_SimpleCNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SigmaModel_SimpleCNN, self).__init__()\n",
    "        if args.dataset == \"CIFAR10\":\n",
    "            self.LN1 = torch.nn.LayerNorm((3,32,32), elementwise_affine=False) \n",
    "            self.conv1 = SigmaConv(args, 3, 32, 5, 3, 2)\n",
    "            self.conv2 = SigmaConv(args, 32, 64, 3, 3, 2)\n",
    "            self.view1 = SigmaView((64, 8, 8), 64 * 8 * 8)\n",
    "            self.fc1 = SigmaLinear(args, 64 * 8 * 8, 512)\n",
    "            self.fc2 = SigmaLinear(args, 512, 10)\n",
    "            \n",
    "        elif args.dataset == \"MNIST\":\n",
    "            if args.architecture == \"lenet\":\n",
    "                self.LN1 = torch.nn.LayerNorm((1,28,28), elementwise_affine=False) \n",
    "                self.conv1 = SigmaConv(args, 1, 32, 5, 2, 3)\n",
    "                self.conv2 = SigmaConv(args, 32, 64, 5, 2, 3)\n",
    "                self.view1 = SigmaView((64, 4, 4), 64 * 4 * 4)\n",
    "                self.fc1 = SigmaLinear(args, 64 * 4 * 4, 512)\n",
    "                self.fc2 = SigmaLinear(args, 512, 10)\n",
    "                \n",
    "        self.forward_params = list()\n",
    "        self.backward_params = list()\n",
    "        for layer in [self.conv1, self.conv2, self.fc1, self.fc2]:\n",
    "            forward_params, backward_params = layer.get_parameters()\n",
    "            self.forward_params += forward_params\n",
    "            self.backward_params += backward_params\n",
    "\n",
    "    def forward(self, x, detach_grad=False, return_activations=True):   \n",
    "        # x = self.LN1(x)\n",
    "        a1 = self.conv1(x, detach_grad)\n",
    "        a2 = self.conv2(a1, detach_grad)\n",
    "        a2 = self.view1(a2, detach_grad)\n",
    "        a3 = self.fc1(a2, detach_grad)\n",
    "        a4 = self.fc2(a3, detach_grad)\n",
    "        return [a1, a2, a3, a4]\n",
    "        \n",
    "    def reverse(self, target, detach_grad=True, return_activations=True):\n",
    "        if target.shape == torch.Size([10]): \n",
    "            target = F.one_hot(target, num_classes=10).float().to(target.device)\n",
    "        b3 = self.fc2.reverse(target, detach_grad)\n",
    "        b2 = self.fc1.reverse(b3, detach_grad)\n",
    "        b2 = self.view1.reverse(b2, detach_grad)\n",
    "        b1 = self.conv2.reverse(b2, detach_grad)\n",
    "        return [b1, b2, b3, target]\n",
    "\n",
    "\n",
    "class SigmaLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SigmaLoss, self).__init__()\n",
    "        self.args = args\n",
    "        self.final_criteria = nn.CrossEntropyLoss()\n",
    "        self.local_criteria = compute_SCL_loss\n",
    "        self.method = args.method\n",
    "        \n",
    "    def forward(self, activations, signals, target, method=\"final\"):\n",
    "        if method == \"local\":\n",
    "            loss = list()\n",
    "            for act, sig in zip(activations[:-1], signals[:-1]):\n",
    "                loss += [self.local_criteria(act, sig, target)]\n",
    "            loss += [self.final_criteria(activations[-1], target)]\n",
    "            return sum(loss), loss[-1].item()\n",
    "        elif method == \"final\":\n",
    "            loss = self.final_criteria(activations[-1], target) * 1.1 - 0.1\n",
    "            return loss, loss.item()\n",
    "        \n",
    "model = SigmaModel_SimpleCNN(args)\n",
    "model.to(device)\n",
    "if args.optimizer == \"SGD\": \n",
    "    forward_optimizer = optim.SGD(model.forward_params, lr=args.lr, momentum=0.9)\n",
    "    backward_optimizer = optim.SGD(model.forward_params, lr=args.lr)\n",
    "elif args.optimizer == \"RMSprop\": forward_optimizer = optim.RMSprop(model.forward_params, lr=args.lr, weight_decay=0)\n",
    "elif args.optimizer == \"Adam\": forward_optimizer = optim.Adam(model.forward_params, lr=args.lr)\n",
    "criteria = SigmaLoss(args)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    signals = model.reverse(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long().to(device), return_activations=True)\n",
    "    \n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"No LN1\")\n",
    "for epoch in range(args.epochs):\n",
    "    \n",
    "    train_loss, train_counter = 0, 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # if batch_idx > 100: continue\n",
    "        \n",
    "        if args.method == \"SIGMA\":\n",
    "            activations = model(data.to(device), detach_grad=True)\n",
    "            signals = model.reverse(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long().to(device), detach_grad=True)\n",
    "            loss, loss_item = criteria(activations, signals, target.to(device), method=\"local\")\n",
    "        elif args.method == \"BP\":\n",
    "            activations = model(data.to(device), detach_grad=False)\n",
    "            loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        forward_optimizer.zero_grad(), backward_optimizer.zero_grad(), loss.backward()\n",
    "        gradient_centralization(model), forward_optimizer.step(), backward_optimizer.step()\n",
    "        train_loss += loss_item * len(data)\n",
    "        train_counter += len(data)\n",
    "\n",
    "    wandb.log({'train_loss': train_loss / train_counter}, step=epoch)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > 100: continue\n",
    "        activations = model(data.to(device), detach_grad=True)\n",
    "        loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        forward_optimizer.zero_grad(), loss.backward(), forward_optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    val_correct, val_loss, val_counter = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            val_counter += len(data)\n",
    "            if args.method == \"SIGMA\":\n",
    "                activations = model(data.to(device), detach_grad=True)\n",
    "                _, loss_item = criteria(activations, signals, target.to(device), method=\"local\")\n",
    "                \n",
    "            elif args.method == \"BP\":\n",
    "                activations = model(data.to(device), detach_grad=True)\n",
    "                _, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "            prediction = activations[-1].detach()\n",
    "            _, predicted = torch.max(prediction, 1)\n",
    "            val_correct += (predicted == target.to(device)).sum().item()\n",
    "            val_loss += loss_item * len(data)\n",
    "\n",
    "    wandb.log({'val_loss': val_loss / val_counter, \n",
    "               'val_acc': val_correct / val_counter, \n",
    "               }, step=epoch)\n",
    "    \n",
    "    print(f\"\"\"Epoch {epoch} | train loss {train_loss / train_counter:.4f} | val loss {val_loss / val_counter:.4f} | val acc {100 * val_correct / val_counter:.4f}\"\"\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), f'./saved_models/{run_name}-{time_stamp}.pt')\n",
    "        \n",
    "\n",
    "# Eval on Test Set by loading the best model \n",
    "model.load_state_dict(torch.load(f'./saved_models/{run_name}-{time_stamp}.pt'))\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "test_loss, test_counter = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(data.to(device), detach_grad=False)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == target.to(device)).sum().item()\n",
    "        loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        test_loss += loss_item * len(data)\n",
    "        test_counter += len(data)\n",
    "\n",
    "wandb.log({'test_loss': test_loss / test_counter,\n",
    "           'test_acc': 100 * correct / test_counter})\n",
    "\n",
    "print(f'Epoch: {epoch}, Test Accuracy: {100 * correct / test_counter:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d937c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Epoch 0 | train loss 1.6212 | val loss 1.2239 | val acc 57.7800\n",
    "# Epoch 1 | train loss 1.1105 | val loss 0.9573 | val acc 67.1100\n",
    "# Epoch 2 | train loss 0.9249 | val loss 0.8443 | val acc 70.5500\n",
    "# Epoch 3 | train loss 0.8030 | val loss 0.7778 | val acc 72.5400\n",
    "# Epoch 4 | train loss 0.7149 | val loss 0.7120 | val acc 75.0400\n",
    "# Epoch 5 | train loss 0.6382 | val loss 0.6874 | val acc 75.8700\n",
    "# Epoch 6 | train loss 0.5760 | val loss 0.6636 | val acc 76.4500\n",
    "# Epoch 7 | train loss 0.5212 | val loss 0.6519 | val acc 77.5800\n",
    "# Epoch 8 | train loss 0.4668 | val loss 0.6548 | val acc 77.4300\n",
    "# Epoch 9 | train loss 0.4243 | val loss 0.6586 | val acc 77.5500\n",
    "# Epoch 10 | train loss 0.3872 | val loss 0.6611 | val acc 78.0400\n",
    "# Epoch 11 | train loss 0.3390 | val loss 0.6800 | val acc 77.9200\n",
    "# Epoch 12 | train loss 0.3089 | val loss 0.7055 | val acc 77.8300\n",
    "# Epoch 13 | train loss 0.2707 | val loss 0.7307 | val acc 77.7800\n",
    "# Epoch 14 | train loss 0.2408 | val loss 0.7463 | val acc 77.9100\n",
    "# Epoch 15 | train loss 0.2150 | val loss 0.7790 | val acc 78.4800"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
