{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffa0c9f-8276-4e73-b6a8-bf33d01039d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"dcf9600e0485401cbb0ddbb0f7be1c70f96b32ef\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "import wandb\n",
    "from sigma_layer import SigmaLinear, SigmaConv, SigmaConvT, SigmaView\n",
    "from utils import get_dataset, gradient_centralization, normalize_along_axis, get_activation_function, compute_SCL_loss\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f367862f-05b5-4c98-b3be-7a36a686df88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: CIFAR10_SIGMA_conv32-actelu_SGD_1_2023\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "No LN1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:12<21:10, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | train loss 1.9366 | val loss 1.6378 | val acc 42.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▊                                          | 2/100 [00:25<21:11, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss 1.7337 | val loss 1.5443 | val acc 45.3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█▎                                         | 3/100 [00:38<20:55, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | train loss 1.6262 | val loss 1.4805 | val acc 47.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                         | 4/100 [00:50<19:58, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | train loss 1.5856 | val loss 1.4413 | val acc 49.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                        | 5/100 [01:01<19:06, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | train loss 1.5380 | val loss 1.4456 | val acc 48.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▌                                        | 6/100 [01:13<18:36, 11.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | train loss 1.4936 | val loss 1.3480 | val acc 52.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|███                                        | 7/100 [01:24<18:05, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | train loss 1.4344 | val loss 1.3465 | val acc 52.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▍                                       | 8/100 [01:36<17:45, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | train loss 1.3892 | val loss 1.3095 | val acc 54.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███▊                                       | 9/100 [01:48<17:54, 11.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | train loss 1.3561 | val loss 1.2774 | val acc 55.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▏                                     | 10/100 [01:59<17:19, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | train loss 1.3094 | val loss 1.2583 | val acc 56.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▌                                     | 11/100 [02:10<16:47, 11.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | train loss 1.2928 | val loss 1.2420 | val acc 56.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████                                     | 12/100 [02:20<16:22, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | train loss 1.2763 | val loss 1.2368 | val acc 56.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▍                                    | 13/100 [02:31<16:03, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | train loss 1.2487 | val loss 1.2276 | val acc 56.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▉                                    | 14/100 [02:42<15:40, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | train loss 1.2370 | val loss 1.2140 | val acc 57.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▎                                   | 15/100 [02:53<15:23, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | train loss 1.2085 | val loss 1.2009 | val acc 58.4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▋                                   | 16/100 [03:03<15:05, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | train loss 1.2104 | val loss 1.1961 | val acc 58.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|███████▏                                  | 17/100 [03:14<15:06, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | train loss 1.2036 | val loss 1.1840 | val acc 59.2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▌                                  | 18/100 [03:25<14:51, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | train loss 1.1919 | val loss 1.1761 | val acc 59.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▉                                  | 19/100 [03:36<14:37, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | train loss 1.1842 | val loss 1.1956 | val acc 58.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▍                                 | 20/100 [03:47<14:28, 10.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | train loss 1.1840 | val loss 1.1870 | val acc 59.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▊                                 | 21/100 [03:58<14:16, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | train loss 1.1563 | val loss 1.1764 | val acc 59.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|█████████▏                                | 22/100 [04:08<13:57, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | train loss 1.1241 | val loss 1.1715 | val acc 59.4900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████▋                                | 23/100 [04:19<13:43, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | train loss 1.1407 | val loss 1.1714 | val acc 59.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████                                | 24/100 [04:29<13:27, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | train loss 1.1092 | val loss 1.1866 | val acc 59.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▌                               | 25/100 [04:40<13:11, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | train loss 1.1405 | val loss 1.1567 | val acc 59.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▉                               | 26/100 [04:50<13:04, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | train loss 1.1196 | val loss 1.1369 | val acc 60.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|███████████▎                              | 27/100 [05:01<13:02, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 | train loss 1.1132 | val loss 1.1402 | val acc 60.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████▊                              | 28/100 [05:12<12:59, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 | train loss 1.1023 | val loss 1.1455 | val acc 60.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|████████████▏                             | 29/100 [05:24<13:03, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | train loss 1.1138 | val loss 1.1466 | val acc 60.7900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████▌                             | 30/100 [05:35<12:53, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 | train loss 1.1009 | val loss 1.1361 | val acc 60.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████                             | 31/100 [05:45<12:28, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 | train loss 1.0905 | val loss 1.1459 | val acc 60.5800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████▍                            | 32/100 [05:56<12:06, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 | train loss 1.1032 | val loss 1.1353 | val acc 60.9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▊                            | 33/100 [06:07<12:04, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 | train loss 1.1115 | val loss 1.1533 | val acc 60.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████▊                            | 33/100 [06:11<12:34, 11.26s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 140\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs)):\n\u001b[1;32m    138\u001b[0m     train_loss, train_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSIGMA\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchvision/transforms/transforms.py:93\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     90\u001b[0m         _log_api_usage_once(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m     95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "# Training scheme group\n",
    "method_parser = parser.add_argument_group(\"Method\")\n",
    "method_parser.add_argument('--method', type=str, default='SIGMA', choices=['SIGMA', 'BP', 'FA'])\n",
    "method_parser.add_argument('--actfunc', type=str, default='elu', choices=['tanh', 'elu', 'relu'])\n",
    "method_parser.add_argument('--conv_dim', type=int, default=32, choices=[32, 64])\n",
    "# Dataset group\n",
    "dataset_parser = parser.add_argument_group('Dataset')\n",
    "dataset_parser.add_argument('--dataset', type=str, default='CIFAR10', choices=['MNIST', 'CIFAR10'])\n",
    "dataset_parser.add_argument('--batchsize', type=int, default=64)\n",
    "dataset_parser.add_argument('--splitratio', type=float, default=0.2)\n",
    "# Training group # LR, optimizer, weight_decay, momentum\n",
    "training_parser = parser.add_argument_group('Training')\n",
    "training_parser.add_argument('--epochs', type=int, default=100)\n",
    "training_parser.add_argument('--lr', type=float, default=1)\n",
    "training_parser.add_argument('--optimizer', type=str, default='SGD', choices=['RMSprop', 'Adam', 'SGD'])\n",
    "# Seed group\n",
    "seed_parser = parser.add_argument_group('Seed')\n",
    "seed_parser.add_argument('--seed', type=int, default=2023)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Set run_name\n",
    "run_name = f\"{args.dataset}_{args.method}_conv{args.conv_dim}-act{args.actfunc}_{args.optimizer}_{args.lr}_{args.seed}\"\n",
    "time_stamp = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Set wandb\n",
    "wandb.init(\n",
    "    project=\"opt-sigma\",\n",
    "    name=run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"algorithm\": args.method,\n",
    "    \"architecture\": \"SimpleCNN\",\n",
    "    \"dataset\": args.dataset,\n",
    "    \"epochs\": args.epochs,\n",
    "    \"lr\": args.lr,\n",
    "    \"optimizer\": args.optimizer,\n",
    "    \"seed\": args.seed,\n",
    "    \"conv_dim\": args.conv_dim,\n",
    "    \"actfunc\": args.actfunc,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(args.seed), np.random.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n",
    "\n",
    "# Get dataset\n",
    "train_loader, val_loader, test_loader = get_dataset(args)\n",
    "\n",
    "class SigmaModel_SimpleCNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SigmaModel_SimpleCNN, self).__init__()\n",
    "        if args.dataset == \"CIFAR10\":\n",
    "            self.LN1 = torch.nn.LayerNorm((3,32,32), elementwise_affine=False) \n",
    "            self.LN2 = torch.nn.LayerNorm((args.conv_dim,16,16), elementwise_affine=False) \n",
    "            self.conv1 = SigmaConvT(3, args.conv_dim, 3, args)\n",
    "            self.conv2 = SigmaConvT(args.conv_dim, 64, 3, args)\n",
    "            self.view1 = SigmaView((64, 8, 8), 64 * 8 * 8)\n",
    "            self.fc1 = SigmaLinear(64 * 8 * 8, 128, args)\n",
    "            self.fc2 = SigmaLinear(128, 10, args)\n",
    "            \n",
    "        elif args.dataset == \"MNIST\":\n",
    "            self.LN1 = torch.nn.LayerNorm((1,28,28), elementwise_affine=False) \n",
    "            self.LN2 = torch.nn.LayerNorm((args.conv_dim,14,14), elementwise_affine=False) \n",
    "            self.conv1 = SigmaConv(1, args.conv_dim, 3, args)\n",
    "            self.conv2 = SigmaConv(args.conv_dim, 64, 3, args)\n",
    "            self.view1 = SigmaView((64, 7, 7), 64 * 7 * 7)\n",
    "            self.fc1 = SigmaLinear(64 * 7 * 7, 128, args)\n",
    "            self.fc2 = SigmaLinear(128, 10, args)\n",
    "                \n",
    "        self.forward_params = list()\n",
    "        self.backward_params = list()\n",
    "        for layer in [self.conv1, self.conv2, self.fc1, self.fc2]:\n",
    "            forward_params, backward_params = layer.get_parameters()\n",
    "            self.forward_params += forward_params\n",
    "            self.backward_params += backward_params\n",
    "\n",
    "    def forward(self, x, detach_grad=False, return_activations=True):   \n",
    "        # x = self.LN1(x)\n",
    "        a1 = self.conv1(x, detach_grad)\n",
    "        a2 = self.conv2(a1, detach_grad)\n",
    "        a2 = self.view1(a2, detach_grad)\n",
    "        a3 = self.fc1(a2, detach_grad)\n",
    "        a4 = self.fc2(a3, detach_grad)\n",
    "        return [a1, a2, a3, a4]\n",
    "        \n",
    "    def reverse(self, target, detach_grad=True, return_activations=True):\n",
    "        if target.shape == torch.Size([10]): \n",
    "            target = F.one_hot(target, num_classes=10).float().to(target.device)\n",
    "        b3 = self.fc2.reverse(target, detach_grad)\n",
    "        b2 = self.fc1.reverse(b3, detach_grad)\n",
    "        b2 = self.view1.reverse(b2, detach_grad)\n",
    "        b1 = self.conv2.reverse(b2, detach_grad)\n",
    "        return [b1, b2, b3, target]\n",
    "\n",
    "\n",
    "class SigmaLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SigmaLoss, self).__init__()\n",
    "        self.args = args\n",
    "        self.final_criteria = nn.CrossEntropyLoss()\n",
    "        self.local_criteria = compute_SCL_loss\n",
    "        self.method = args.method\n",
    "        \n",
    "    def forward(self, activations, signals, target, method=\"final\"):\n",
    "        if method == \"local\":\n",
    "            loss = list()\n",
    "            for act, sig in zip(activations[:-1], signals[:-1]):\n",
    "                loss += [self.local_criteria(act, sig, target)]\n",
    "            loss += [self.final_criteria(activations[-1], target)]\n",
    "            return sum(loss), loss[-1].item()\n",
    "        elif method == \"final\":\n",
    "            loss = self.final_criteria(activations[-1], target) * 1.1 - 0.1\n",
    "            return loss, loss.item()\n",
    "        \n",
    "model = SigmaModel_SimpleCNN(args)\n",
    "model.to(device)\n",
    "if args.optimizer == \"SGD\": \n",
    "    forward_optimizer = optim.SGD(model.forward_params, lr=args.lr)\n",
    "    backward_optimizer = optim.SGD(model.forward_params, lr=args.lr)\n",
    "elif args.optimizer == \"RMSprop\": forward_optimizer = optim.RMSprop(model.forward_params, lr=args.lr, weight_decay=0)\n",
    "elif args.optimizer == \"Adam\": forward_optimizer = optim.Adam(model.forward_params, lr=args.lr)\n",
    "criteria = SigmaLoss(args)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    signals = model.reverse(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long().to(device), return_activations=True)\n",
    "    \n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"No LN1\")\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    \n",
    "    train_loss, train_counter = 0, 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > 100: continue\n",
    "        if args.method == \"SIGMA\":\n",
    "            activations = model(data.to(device), detach_grad=True)\n",
    "            # signals = model.reverse(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long().to(device), detach_grad=True)\n",
    "            loss, loss_item = criteria(activations, signals, target.to(device), method=\"local\")\n",
    "        elif args.method == \"BP\":\n",
    "            activations = model(data.to(device), detach_grad=False)\n",
    "            loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        forward_optimizer.zero_grad(), backward_optimizer.zero_grad(), loss.backward()\n",
    "        gradient_centralization(model), forward_optimizer.step(), backward_optimizer.step()\n",
    "        train_loss += loss_item * len(data)\n",
    "        train_counter += len(data)\n",
    "\n",
    "    wandb.log({'train_loss': train_loss / train_counter}, step=epoch)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > 100: continue\n",
    "        activations = model(data.to(device), detach_grad=True)\n",
    "        loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        forward_optimizer.zero_grad(), loss.backward(), forward_optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    val_correct, val_loss, val_counter = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            val_counter += len(data)\n",
    "            if args.method == \"SIGMA\":\n",
    "                activations = model(data.to(device), detach_grad=True)\n",
    "                _, loss_item = criteria(activations, signals, target.to(device), method=\"local\")\n",
    "                \n",
    "            elif args.method == \"BP\":\n",
    "                activations = model(data.to(device), detach_grad=True)\n",
    "                _, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "            prediction = activations[-1].detach()\n",
    "            _, predicted = torch.max(prediction, 1)\n",
    "            val_correct += (predicted == target.to(device)).sum().item()\n",
    "            val_loss += loss_item * len(data)\n",
    "\n",
    "    wandb.log({'val_loss': val_loss / val_counter, \n",
    "               'val_acc': val_correct / val_counter, \n",
    "               }, step=epoch)\n",
    "    \n",
    "    print(f\"\"\"Epoch {epoch} | train loss {train_loss / train_counter:.4f} | val loss {val_loss / val_counter:.4f} | val acc {100 * val_correct / val_counter:.4f}\"\"\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), f'./saved_models/{run_name}-{time_stamp}.pt')\n",
    "        \n",
    "\n",
    "# Eval on Test Set by loading the best model \n",
    "model.load_state_dict(torch.load(f'./saved_models/{run_name}-{time_stamp}.pt'))\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "test_loss, test_counter = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(data.to(device), detach_grad=False)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == target.to(device)).sum().item()\n",
    "        loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        test_loss += loss_item * len(data)\n",
    "        test_counter += len(data)\n",
    "\n",
    "wandb.log({'test_loss': test_loss / test_counter,\n",
    "           'test_acc': 100 * correct / test_counter})\n",
    "\n",
    "print(f'Epoch: {epoch}, Test Accuracy: {100 * correct / test_counter:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c9b889-e264-4be3-8048-a51fb2f75eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Epoch 0 | train loss 1.9373 | val loss 1.6394 | val acc 41.9600\n",
    "  2%|▊                                          | 2/100 [00:24<19:51, 12.16s/it]\n",
    "Epoch 1 | train loss 1.7356 | val loss 1.5472 | val acc 45.1800\n",
    "  3%|█▎                                         | 3/100 [00:36<19:46, 12.23s/it]\n",
    "Epoch 2 | train loss 1.6277 | val loss 1.4846 | val acc 47.2700\n",
    "  4%|█▋                                         | 4/100 [00:48<19:26, 12.15s/it]\n",
    "Epoch 3 | train loss 1.5847 | val loss 1.4396 | val acc 49.5900\n",
    "  5%|██▏                                        | 5/100 [01:00<19:23, 12.25s/it]\n",
    "Epoch 4 | train loss 1.5357 | val loss 1.4488 | val acc 48.9100\n",
    "  6%|██▌                                        | 6/100 [01:13<19:19, 12.34s/it]\n",
    "Epoch 5 | train loss 1.4928 | val loss 1.3468 | val acc 52.9600\n",
    "  7%|███                                        | 7/100 [01:26<19:26, 12.54s/it]\n",
    "Epoch 6 | train loss 1.4379 | val loss 1.3484 | val acc 52.8400\n",
    "  8%|███▍                                       | 8/100 [01:39<19:33, 12.75s/it]\n",
    "Epoch 7 | train loss 1.3875 | val loss 1.3066 | val acc 54.5600\n",
    "  9%|███▊                                       | 9/100 [01:51<19:06, 12.60s/it]\n",
    "Epoch 8 | train loss 1.3699 | val loss 1.2766 | val acc 55.5900\n",
    " 10%|████▏                                     | 10/100 [02:05<19:19, 12.88s/it]\n",
    "Epoch 9 | train loss 1.3236 | val loss 1.2622 | val acc 55.9400\n",
    " 11%|████▌                                     | 11/100 [02:19<19:49, 13.36s/it]\n",
    "Epoch 10 | train loss 1.3133 | val loss 1.2524 | val acc 56.2600\n",
    " 12%|█████                                     | 12/100 [02:34<20:07, 13.72s/it]\n",
    "Epoch 11 | train loss 1.2969 | val loss 1.2430 | val acc 57.1900\n",
    " 13%|█████▍                                    | 13/100 [02:48<20:07, 13.88s/it]\n",
    "Epoch 12 | train loss 1.2637 | val loss 1.2338 | val acc 56.9000\n",
    " 14%|█████▉                                    | 14/100 [03:05<21:02, 14.68s/it]\n",
    "Epoch 13 | train loss 1.2559 | val loss 1.2223 | val acc 58.0000\n",
    " 15%|██████▎                                   | 15/100 [03:21<21:37, 15.27s/it]\n",
    "Epoch 14 | train loss 1.2275 | val loss 1.2109 | val acc 58.2600\n",
    " 16%|██████▋                                   | 16/100 [03:37<21:25, 15.30s/it]\n",
    "Epoch 15 | train loss 1.2327 | val loss 1.2032 | val acc 58.4900\n",
    " 17%|███████▏                                  | 17/100 [03:53<21:35, 15.60s/it]\n",
    "Epoch 16 | train loss 1.2229 | val loss 1.1895 | val acc 59.2200\n",
    " 18%|███████▌                                  | 18/100 [04:09<21:31, 15.75s/it]\n",
    "Epoch 17 | train loss 1.2117 | val loss 1.1838 | val acc 59.1000\n",
    " 19%|███████▉                                  | 19/100 [04:24<20:52, 15.46s/it]\n",
    "Epoch 18 | train loss 1.2045 | val loss 1.1971 | val acc 59.0200\n",
    " 20%|████████▍                                 | 20/100 [04:39<20:19, 15.24s/it]\n",
    "Epoch 19 | train loss 1.1952 | val loss 1.1982 | val acc 59.2300\n",
    " 21%|████████▊                                 | 21/100 [04:55<20:20, 15.45s/it]\n",
    "Epoch 20 | train loss 1.1688 | val loss 1.1887 | val acc 59.2200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aa9f2e-4bd0-4053-a931-ed6f411b06a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(S1)\n",
    "plt.plot(S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe052f76-15fc-4165-8157-a46c475787e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W12 = W1@W2\n",
    "plt.imshow(W12), print(W12.min(), W12.max()), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0475fd12-f264-48f3-9284-4cfaea965bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "val_correct, val_loss, val_counter = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        val_counter += len(data)\n",
    "        if args.method == \"SIGMA\":\n",
    "            activations = model(data.to(device), detach_grad=True)\n",
    "            _, loss_item = criteria(activations, signals, target.to(device), method=\"local\")\n",
    "        elif args.method == \"BP\":\n",
    "            activations = model(data.to(device), detach_grad=True)\n",
    "            _, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        prediction = activations[-1].detach()\n",
    "        _, predicted = torch.max(prediction, 1)\n",
    "        val_correct += (predicted == target.to(device)).sum().item()\n",
    "        val_loss += loss_item * len(data)\n",
    "\n",
    "wandb.log({'val_loss': val_loss / val_counter, \n",
    "           'val_acc': val_correct / val_counter, \n",
    "           }, step=epoch)\n",
    "s\n",
    "print(f\"\"\"Epoch {epoch} | train loss {train_loss / train_counter:.4f} | val loss {val_loss / val_counter:.4f} | val acc {100 * val_correct / val_counter:.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1713d3e-0226-47a8-ab3f-5fd685b7ea38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
