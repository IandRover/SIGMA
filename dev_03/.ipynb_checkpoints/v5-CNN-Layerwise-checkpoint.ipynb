{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(283.8713) tensor(0.7854) tensor(142.7051) tensor(0.7888)\n",
      "tensor([54.7917, 51.2640, 49.5042, 48.6735, 46.3613, 43.9471, 42.0677, 41.4580,\n",
      "        38.5330, 29.3407])\n",
      "Epoch: 0, Test Accuracy: 95.15%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(2024)\n",
    "# torch.manual_seed(2023)\n",
    "HIDDEN_DIM = 256\n",
    "USE_SIGMA = 1\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "def gradient_centralization():\n",
    "    with torch.no_grad():\n",
    "        for p1, p2 in forward_model.named_parameters():\n",
    "            if \"bias\" in p1 or p2.grad is None: continue\n",
    "            if len(p2.shape) == 2: p2.grad -= p2.grad.mean(dim=1,keepdim=True)\n",
    "            elif len(p2.shape) == 4: p2.grad -= p2.grad.mean(dim=[1,2,3],keepdim=True) \n",
    "\n",
    "class ForwardModel(nn.Module):\n",
    "    def __init__(self, conv_dim=32, hidden_dim=128, use_sigma=True):\n",
    "        super(ForwardModel, self).__init__()\n",
    "        self.use_sigma = use_sigma\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_dim = conv_dim\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, conv_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(conv_dim, conv_dim, kernel_size=3, padding=1)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(conv_dim * 7 * 7, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 10)\n",
    "\n",
    "        self.LN1 = nn.LayerNorm((conv_dim, 14, 14), elementwise_affine=False)\n",
    "        self.LN2 = nn.LayerNorm((1, 28, 28), elementwise_affine=False)\n",
    "\n",
    "        self.fc2.weight.data.zero_()\n",
    "        self.fc2.bias.data.zero_()\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.conv2.bias.data.zero_()\n",
    "        self.conv1.bias.data.zero_()\n",
    "        \n",
    "        self.act = F.elu\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = self.LN2(x)\n",
    "\n",
    "        a1 = self.act(self.conv1(x.detach()))\n",
    "        a1 = self.max_pool(a1)\n",
    "        a1 = self.LN1(a1)\n",
    "\n",
    "        a2 = self.act(self.conv2(a1.detach()))\n",
    "        a2 = self.max_pool(a2)\n",
    "        a2 = a2.view(-1, self.conv_dim * 7 * 7)\n",
    "\n",
    "        a3 = self.act(self.fc1(a2.detach()))\n",
    "\n",
    "        a4 = self.fc2(a3.detach())\n",
    "        self.penultimate_feature = a3.detach()\n",
    "\n",
    "        return a1, a2, a3, a4\n",
    "\n",
    "    def forward_logits(self):\n",
    "        return self.fc2(self.penultimate_feature)\n",
    "\n",
    "class BackwardModel(nn.Module):\n",
    "    def __init__(self, conv_dim=32,  hidden_dim=128):\n",
    "        super(BackwardModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        alpha = 1\n",
    "        self.alpha = alpha\n",
    "        self.fc1 = nn.Linear(10, hidden_dim * alpha, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_dim * alpha, self.conv_dim * 7 * 7 * alpha, bias=False)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv1 = nn.Conv2d(self.conv_dim * alpha, self.conv_dim * alpha, kernel_size=5, padding=2, bias=False)\n",
    "        self.act = F.elu\n",
    "        \n",
    "        self.BN1 = torch.nn.BatchNorm2d(self.conv_dim * alpha)\n",
    "        self.BN2 = torch.nn.BatchNorm1d(self.conv_dim * 7 * 7 * alpha)\n",
    "        self.BN3 = torch.nn.BatchNorm1d(hidden_dim * alpha)\n",
    "\n",
    "    def forward(self, t, use_act_derivative=False):\n",
    "        device = t.device\n",
    "        t = F.one_hot(t, num_classes=10).float().to(device)\n",
    "        s3 = self.act(self.BN3(self.fc1(t.detach())))\n",
    "\n",
    "        s2 = self.act(self.BN2(self.fc2(s3.detach()))  )\n",
    "        \n",
    "        s2_ = s2.view(-1, self.conv_dim * self.alpha, 7, 7)\n",
    "        s2_ = self.upsample1(s2_)        \n",
    "        s1 = self.act(self.BN1(self.conv1(s2_.detach())))\n",
    "        \n",
    "        return s1[:, :self.conv_dim], s2[:,:self.conv_dim*7*7], s3[:, :self.hidden_dim]\n",
    "\n",
    "def normal(x): \n",
    "    if len(x.shape) == 2: return x / x.std(dim=1, keepdim=True)\n",
    "    if len(x.shape) == 4: return x / x.std(dim=[1,2,3], keepdim=True)\n",
    "    # if len(x.shape) == 2: return x / ( 1 + x.std(dim=1, keepdim=True))\n",
    "    # if len(x.shape) == 4: return x / ( 1 + x.std(dim=[1,2,3], keepdim=True))\n",
    "\n",
    "def standard(x): \n",
    "    if len(x.shape) == 2: \n",
    "        x = x - x.mean(dim=1, keepdim=True)\n",
    "        return x / x.std(dim=1, keepdim=True)\n",
    "    if len(x.shape) == 4: \n",
    "        x = x - x.mean(dim=[1,2,3], keepdim=True)\n",
    "        return x / x.std(dim=[1,2,3], keepdim=True)\n",
    "\n",
    "def single_loss(a, s):\n",
    "    a = normal(a)\n",
    "    s = normal(s)\n",
    "    return F.mse_loss(a, s)\n",
    "\n",
    "def sigma_loss(a1, a2, a3, a4, s1, s2, s3, t):\n",
    "    loss1 = single_loss(a1,s1)\n",
    "    loss2 = single_loss(a2,s2)\n",
    "    loss3 = single_loss(a3,s3)\n",
    "    loss4 = criteria(a4, t)\n",
    "    loss = loss1 + loss2 + loss3 + loss4\n",
    "    return loss, loss1.item(), loss2.item(), loss3.item(), loss4.item()\n",
    "\n",
    "def gradient_centralization_B():\n",
    "    with torch.no_grad():\n",
    "        for p1, p2 in backward_model.named_parameters():\n",
    "            if \"bias\" in p1 or p2.grad is None: continue\n",
    "            if len(p2.shape) == 2: p2.grad -= p2.grad.mean(dim=1,keepdim=True)\n",
    "            elif len(p2.shape) == 4: p2.grad -= p2.grad.mean(dim=[1,2,3],keepdim=True) \n",
    "\n",
    "# Initialize the models\n",
    "forward_model = ForwardModel(use_sigma=USE_SIGMA, conv_dim=64, hidden_dim=128)\n",
    "backward_model = BackwardModel(conv_dim=64, hidden_dim=128)\n",
    "\n",
    "forward_model.to(device)\n",
    "backward_model.to(device)\n",
    "\n",
    "# Define the optimizers\n",
    "forward_optimizer = optim.RMSprop(forward_model.parameters(),     lr=0.001)\n",
    "backward_optimizer = optim.RMSprop(backward_model.parameters(),   lr=0.001)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "    \n",
    "def small_loss(s):\n",
    "    s = s.reshape(10,-1)\n",
    "    s = s / s.norm(dim=1, keepdim=True)\n",
    "    target = torch.eye(10)\n",
    "    return F.mse_loss(s@s.T, target)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    t1, t2, t3 = backward_model(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long(), use_act_derivative=False)\n",
    "    \n",
    "import torch\n",
    "import numpy as np\n",
    "    \n",
    "for epoch in range(30):\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        if batch_idx > 100: continue\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        a1, a2, a3, a4 = forward_model(data)\n",
    "        # t1, t2, t3 = backward_model(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long(), use_act_derivative=False)\n",
    "        s1, s2, s3 = t1[target], t2[target], t3[target]\n",
    "        loss, l1, l2, l3, l4 = sigma_loss(a1, a2, a3, a4, s1, s2, s3, target)\n",
    "        forward_optimizer.zero_grad(), backward_optimizer.zero_grad()\n",
    "        loss.backward(), gradient_centralization(), gradient_centralization_B()\n",
    "        forward_optimizer.step()\n",
    "        backward_optimizer.step()\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > 100: continue\n",
    "        a1, a2, a3, a4 = forward_model(data.to(device))\n",
    "        loss = criteria(a4, target.to(device))\n",
    "        forward_optimizer.zero_grad(), loss.backward(), forward_optimizer.step()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        print(t1.norm(), t1.std(), t2.norm(), t2.std())\n",
    "        print(torch.svd(t2)[1].detach())\n",
    "    \n",
    "    forward_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            _, _, _, outputs = forward_model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    print(f'Epoch: {epoch}, Test Accuracy: {100 * correct / total}%')\n",
    "    forward_model.train()\n",
    "    \n",
    "    # assert 0 == 1\n",
    "    # if epoch in [19]:\n",
    "    #     torch.save(forward_model.state_dict(), \"./saved_models/v4-20EP-F.pt\") #Epoch: 46, Test Accuracy: 98.05%; Epoch: 47, Test Accuracy: 98.16%; Epoch: 48, Test Accuracy: 98.01%; Epoch: 49, Test Accuracy: 98.13%\n",
    "    #     torch.save(backward_model.state_dict(),\"./saved_models/v4-20EP-B.pt\") #Epoch: 46, Test Accuracy: 98.05%; Epoch: 47, Test Accuracy: 98.16%; Epoch: 48, Test Accuracy: 98.01%; Epoch: 49, Test Accuracy: 98.13%\n",
    "\n",
    "# conv_dim 64; s1, s2, s3 use BN before act.\n",
    "# Epoch: 0, Test Accuracy: 95.15%\n",
    "# Epoch: 1, Test Accuracy: 96.6%\n",
    "# Epoch: 2, Test Accuracy: 97.13%\n",
    "# Epoch: 3, Test Accuracy: 97.89%\n",
    "# Epoch: 4, Test Accuracy: 97.6%\n",
    "# Epoch: 5, Test Accuracy: 98.06%\n",
    "# Epoch: 6, Test Accuracy: 98.1%\n",
    "# Epoch: 7, Test Accuracy: 98.11%\n",
    "# Epoch: 8, Test Accuracy: 98.54%\n",
    "# Epoch: 9, Test Accuracy: 98.47%\n",
    "# Epoch: 10, Test Accuracy: 98.44%\n",
    "# Epoch: 11, Test Accuracy: 98.41%\n",
    "# Epoch: 12, Test Accuracy: 98.59%\n",
    "# Epoch: 13, Test Accuracy: 98.87%\n",
    "# Epoch: 14, Test Accuracy: 98.59%\n",
    "# Epoch: 15, Test Accuracy: 98.65%\n",
    "# Epoch: 16, Test Accuracy: 98.52%\n",
    "# Epoch: 17, Test Accuracy: 98.58%\n",
    "# Epoch: 18, Test Accuracy: 98.76%\n",
    "# Epoch: 19, Test Accuracy: 98.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch: 0, Test Accuracy: 92.02%\n",
    "Epoch: 1, Test Accuracy: 92.04%\n",
    "Epoch: 2, Test Accuracy: 91.61%\n",
    "Epoch: 3, Test Accuracy: 90.56%\n",
    "Epoch: 4, Test Accuracy: 90.93%\n",
    "Epoch: 5, Test Accuracy: 90.65%\n",
    "Epoch: 6, Test Accuracy: 90.6%\n",
    "Epoch: 7, Test Accuracy: 90.62%\n",
    "Epoch: 8, Test Accuracy: 90.27%\n",
    "Epoch: 9, Test Accuracy: 90.04%\n",
    "Epoch: 10, Test Accuracy: 89.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(230.1083) tensor(0.6460) tensor(54.8109) tensor(0.2087)\n",
      "tensor([41.3883, 15.7216, 14.2806, 13.1660, 12.3549, 11.4565, 11.1876, 10.4063,\n",
      "         9.1648,  8.0901])\n"
     ]
    }
   ],
   "source": [
    "    with torch.no_grad():\n",
    "        print(t1.norm(), t1.std(), t2.norm(), t2.std())\n",
    "        print(torch.svd(t2)[1].detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(283.8713), tensor(0.7854), tensor(142.7051), tensor(0.7888))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.norm(), t1.std(), t2.norm(), t2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41.3883, 15.7216, 14.2806, 13.1660, 12.3549, 11.4565, 11.1876, 10.4063,\n",
       "         9.1648,  8.0901])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.svd(t2)[1].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
