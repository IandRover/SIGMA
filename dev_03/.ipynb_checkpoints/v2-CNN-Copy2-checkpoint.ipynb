{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.019, Loss4: 0.044\n",
      "Epoch: 0, Test Accuracy: 88.27%\n",
      "norm of fc3 2.0\n",
      "Epoch: 1, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.019, Loss4: 0.041\n",
      "Epoch: 1, Test Accuracy: 90.26%\n",
      "norm of fc3 2.5\n",
      "Epoch: 2, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.017, Loss4: 0.032\n",
      "Epoch: 2, Test Accuracy: 90.85%\n",
      "norm of fc3 2.9\n",
      "Epoch: 3, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.017, Loss4: 0.031\n",
      "Epoch: 3, Test Accuracy: 91.23%\n",
      "norm of fc3 3.1\n",
      "Epoch: 4, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.016, Loss4: 0.036\n",
      "Epoch: 4, Test Accuracy: 91.05%\n",
      "norm of fc3 3.3\n",
      "Epoch: 5, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.016, Loss4: 0.033\n",
      "Epoch: 5, Test Accuracy: 91.38%\n",
      "norm of fc3 3.4\n",
      "Epoch: 6, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.015, Loss4: 0.040\n",
      "Epoch: 6, Test Accuracy: 91.19%\n",
      "norm of fc3 3.5\n",
      "Epoch: 7, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.015, Loss4: 0.034\n",
      "Epoch: 7, Test Accuracy: 91.27%\n",
      "norm of fc3 3.6\n",
      "Epoch: 8, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.015, Loss4: 0.027\n",
      "Epoch: 8, Test Accuracy: 91.59%\n",
      "norm of fc3 3.7\n",
      "Epoch: 9, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.015, Loss4: 0.035\n",
      "Epoch: 9, Test Accuracy: 91.42%\n",
      "norm of fc3 3.8\n",
      "Epoch: 10, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.015, Loss4: 0.039\n",
      "Epoch: 10, Test Accuracy: 91.46%\n",
      "norm of fc3 3.8\n",
      "Epoch: 11, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.015, Loss4: 0.030\n",
      "Epoch: 11, Test Accuracy: 91.29%\n",
      "norm of fc3 3.9\n",
      "Epoch: 12, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.014, Loss4: 0.035\n",
      "Epoch: 12, Test Accuracy: 91.63%\n",
      "norm of fc3 3.9\n",
      "Epoch: 13, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.014, Loss4: 0.034\n",
      "Epoch: 13, Test Accuracy: 91.33%\n",
      "norm of fc3 4.0\n",
      "Epoch: 14, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.014, Loss4: 0.036\n",
      "Epoch: 14, Test Accuracy: 91.66%\n",
      "norm of fc3 4.0\n",
      "Epoch: 15, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.013, Loss4: 0.029\n",
      "Epoch: 15, Test Accuracy: 91.85%\n",
      "norm of fc3 4.1\n",
      "Epoch: 16, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.013, Loss4: 0.033\n",
      "Epoch: 16, Test Accuracy: 91.53%\n",
      "norm of fc3 4.1\n",
      "Epoch: 17, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.013, Loss4: 0.032\n",
      "Epoch: 17, Test Accuracy: 91.49%\n",
      "norm of fc3 4.1\n",
      "Epoch: 18, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.013, Loss4: 0.029\n",
      "Epoch: 18, Test Accuracy: 91.43%\n",
      "norm of fc3 4.1\n",
      "Epoch: 19, Batch: 937, Loss1: 0.001, Loss2: 0.002, Loss3: 0.013, Loss4: 0.035\n",
      "Epoch: 19, Test Accuracy: 91.67%\n",
      "norm of fc3 4.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(2024)\n",
    "HIDDEN_DIM = 256\n",
    "USE_SIGMA = 1\n",
    "# USE_BP = 1\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class ForwardModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, use_sigma=True):\n",
    "        super(ForwardModel, self).__init__()\n",
    "        self.use_sigma = use_sigma\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 10)\n",
    "        \n",
    "        self.LN1 = torch.nn.LayerNorm((32,14,14), elementwise_affine=False)\n",
    "        self.LN2 = torch.nn.LayerNorm((1,28,28), elementwise_affine=False)\n",
    "\n",
    "        if use_sigma:\n",
    "            \n",
    "            self.fc2.weight.data.zero_()\n",
    "            self.fc2.bias.data.zero_()\n",
    "            self.fc1.bias.data.zero_()\n",
    "            self.conv2.bias.data.zero_()\n",
    "            self.conv1.bias.data.zero_()\n",
    "            \n",
    "            self.fc2.weight.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if self.use_sigma:\n",
    "            x = x.view(-1, 1, 28, 28)\n",
    "            x = self.LN2(x)\n",
    "            a1 = F.elu(self.conv1(x.detach()))\n",
    "            a1 = self.max_pool(a1)\n",
    "            a1 = self.LN1(a1)\n",
    "            \n",
    "            a2 = F.elu(self.conv2(a1.detach()))\n",
    "            a2 = self.max_pool(a2)\n",
    "            a2 = a2.view(-1, 32 * 7 * 7)\n",
    "            \n",
    "            a3 = F.elu(self.fc1(a2.detach()))\n",
    "            \n",
    "            a4 = self.fc2(a3.detach())\n",
    "            self.penultimate_feature = a3.detach()\n",
    "            \n",
    "            return a1, a2, a3, a4\n",
    "            \n",
    "        else:\n",
    "            x = x.view(-1, 1, 28, 28)\n",
    "            x = F.elu(self.conv1(x))\n",
    "            x = self.max_pool(x)\n",
    "            x = F.elu(self.conv2(x))\n",
    "            x = self.max_pool(x)\n",
    "            x = x.view(-1, 32 * 7 * 7)\n",
    "            x = F.elu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return None, None, None, x\n",
    "\n",
    "    def forward_logits(self):\n",
    "        return self.fc2(self.penultimate_feature)\n",
    "\n",
    "class BackwardModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=128):\n",
    "        super(BackwardModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(10, hidden_dim, bias=False)\n",
    "        self.fc1_2 = nn.Linear(10, hidden_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 32 * 7 * 7, bias=False)\n",
    "        self.fc2_2 = nn.Linear(hidden_dim, 32 * 7 * 7, bias=False)\n",
    "\n",
    "        # Transposed Convolutions (Deconvolutions)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=5, padding=2, bias=False)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=5, padding=2, bias=False)\n",
    "        \n",
    "    def forward(self, t, use_act_derivative=False):\n",
    "        device = t.device\n",
    "        t = F.one_hot(t, num_classes=10).float().to(device)\n",
    "        \n",
    "        if use_act_derivative:\n",
    "            \n",
    "            s3 = (F.relu(self.fc1(t.detach())) + F.relu(self.fc1_2(t.detach()))) / 2 * a3.sign().detach()\n",
    "            s2 = (F.relu(self.fc2(s3.detach())) + F.relu(self.fc2_2(s3.detach())) ) / 2  * a2.sign().detach()\n",
    "            s1 = s2.view(-1, 32, 7, 7).detach()\n",
    "            s1 = self.upsample1(s1)\n",
    "            s1 = (F.relu(self.conv1(s1))+ F.relu(self.conv1_2(s1))) / 2  * a1.sign().detach()\n",
    "            \n",
    "            # s3 = F.relu(self.fc1(t.detach())) * a3.sign().detach()\n",
    "            # s2 = F.relu(self.fc2(s3.detach()))  * a2.sign().detach()\n",
    "            # s1 = s2.view(-1, 32, 7, 7).detach()\n",
    "            # s1 = self.upsample1(s1)\n",
    "            # s1 = F.relu(self.conv1(s1))  * a1.sign().detach()\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            s3 = (F.relu(self.fc1(t.detach())) + F.relu(self.fc1(t.detach()))) / 2\n",
    "            \n",
    "            s2 = (F.relu(self.fc2(s3.detach())) + F.relu(self.fc2_2(s3.detach())) ) / 2\n",
    "            \n",
    "            s1 = s2.view(-1, 32, 7, 7).detach()\n",
    "            s1 = self.upsample1(s1)\n",
    "            s1 = (F.relu(self.conv1(s1))+ F.relu(self.conv1_2(s1))) / 2\n",
    "            \n",
    "        return s1, s2, s3\n",
    "    \n",
    "def normal(x): \n",
    "    return x / (x.norm()+1e-6)\n",
    "    \n",
    "def normal_2d(x): \n",
    "    return x / (x.norm()+1e-6)\n",
    "\n",
    "# Define the loss functions\n",
    "WEIGHT_DECAY = 0.000000\n",
    "def sigma_loss(a1, a2, a3, a4, s1, s2, s3, t):\n",
    "    assert a1.shape == s1.shape, f\"shape {a1.shape} does not align with shape {s1.shape}\"\n",
    "    assert a2.shape == s2.shape, f\"shape {a2.shape} does not align with shape {s2.shape}\"\n",
    "    assert a3.shape == s3.shape, f\"shape {a3.shape} does not align with shape {s3.shape}\"\n",
    "    loss1 = F.mse_loss(normal_2d(a1), normal_2d(s1))\n",
    "    loss2 = F.mse_loss(normal(a2), normal(s2))\n",
    "    loss3 = F.mse_loss(normal(a3), normal(s3))\n",
    "    loss4 = F.mse_loss(a4, torch.nn.functional.one_hot(t, num_classes=10).float().to(t.device))\n",
    "    loss = loss1+loss2+loss3+loss4\n",
    "    # + (a1.norm()+a2.norm()+a3.norm()+s1.norm()+s2.norm()+s3.norm())*WEIGHT_DECAY\n",
    "    return loss, loss1.item(), loss2.item(), loss3.item(), loss4.item(), \n",
    "\n",
    "def sigma_loss_head(a4, t):\n",
    "    return F.mse_loss(a4, torch.nn.functional.one_hot(t, num_classes=10).float().to(t.device))\n",
    "                       \n",
    "def bp_loss(a, b):\n",
    "    return criteria(a,b)\n",
    "\n",
    "# Initialize the models\n",
    "forward_model = ForwardModel(use_sigma=USE_SIGMA)\n",
    "backward_model = BackwardModel()\n",
    "\n",
    "# Define the optimizers: momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False\n",
    "forward_optimizer = optim.SGD(forward_model.parameters(), lr=0.1)\n",
    "backward_optimizer = optim.SGD(backward_model.parameters(), lr=0.1)\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        if USE_SIGMA: \n",
    "\n",
    "            a1, a2, a3, a4 = forward_model(data)\n",
    "            s1, s2, s3 = backward_model(target, use_act_derivative=True)\n",
    "            loss, l1, l2, l3, l4 = sigma_loss(a1, a2, a3, a4, s1, s2, s3, target)\n",
    "\n",
    "            # Update parameters\n",
    "            forward_optimizer.zero_grad()\n",
    "            backward_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            forward_optimizer.step()\n",
    "            backward_optimizer.step()\n",
    "\n",
    "            # Update the linear head again\n",
    "            x = forward_model.forward_logits()\n",
    "            loss = sigma_loss_head(x, target)\n",
    "            # loss = criteria(x, target)\n",
    "            forward_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            forward_optimizer.step()\n",
    "                       \n",
    "        else:\n",
    "            _, _, _, x = forward_model(data)\n",
    "            loss = criteria(x, target)\n",
    "            forward_optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            forward_optimizer.step()  \n",
    "            \n",
    "    # Print statistics\n",
    "    print(f'Epoch: {epoch}, Batch: {batch_idx}, Loss1: {l1*100:.3f}, Loss2: {l2*100:.3f}, Loss3: {l3*100:.3f}, Loss4: {l4:.3f}')\n",
    "\n",
    "    # Evaluate on test set\n",
    "    forward_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            _, _, _, outputs = forward_model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    print(f'Epoch: {epoch}, Test Accuracy: {100 * correct / total}%')\n",
    "    print(f\"norm of fc3 {forward_model.fc2.weight.data.norm():.1f}\")\n",
    "    forward_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 8, 7],\n",
       "        [2, 7, 9],\n",
       "        [3, 8, 5],\n",
       "        [4, 8, 1],\n",
       "        [5, 3, 8],\n",
       "        [6, 8, 5],\n",
       "        [7, 9, 3],\n",
       "        [8, 3, 2],\n",
       "        [9, 4, 0],\n",
       "        [0, 8, 1],\n",
       "        [1, 2, 8],\n",
       "        [2, 7, 8],\n",
       "        [3, 2, 0],\n",
       "        [4, 7, 6],\n",
       "        [6, 5, 8],\n",
       "        [6, 2, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(a4, k=3, dim=-1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for data, target in test_loader: break\n",
    "\n",
    "a1, a2, a3, a4 = forward_model(data)\n",
    "s1, s2, s3 = backward_model(target, use_act_derivative=True)\n",
    "# p = torch.argmax(a4, dim=-1)\n",
    "p = torch.topk(a4, k=3, dim=-1).indices\n",
    "v = torch.topk(a4, k=3, dim=-1).values\n",
    "\n",
    "for i1 in range(16):\n",
    "    \n",
    "    if p[i1][0] != target[i1]:\n",
    "        \n",
    "        i2 = 11\n",
    "        print(f\"prediction: {p[i1]}, {v[i1].detach()} groudtruth: {target[i1]}\")\n",
    "\n",
    "        plt.figure()\n",
    "        fig, axes = plt.subplots(1,2)\n",
    "\n",
    "        X = a1.detach().cpu()\n",
    "        # X = X / X.norm()\n",
    "        X = X.numpy()\n",
    "\n",
    "\n",
    "        axes[0].imshow(X[i1,i2], vmin=-1, vmax=1)\n",
    "\n",
    "        X = s1.detach().cpu()\n",
    "        # X = X / X.norm()\n",
    "        X = X.numpy()\n",
    "        print(X[i1,i2].min(), X[i1,i2].max())\n",
    "\n",
    "        axes[1].imshow(X[i1,i2], vmin=-0.0, vmax=0.03)\n",
    "        plt.pause(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6677, 0.1409, 0.1319],\n",
       "        [0.3594, 0.2306, 0.1896],\n",
       "        [0.3866, 0.2124, 0.1795],\n",
       "        [0.8142, 0.1682, 0.1573],\n",
       "        [0.6672, 0.1992, 0.1568],\n",
       "        [0.8042, 0.1464, 0.1125],\n",
       "        [0.5750, 0.3306, 0.1425],\n",
       "        [0.5981, 0.2090, 0.1507],\n",
       "        [0.6315, 0.4538, 0.1122],\n",
       "        [0.7532, 0.1984, 0.1276],\n",
       "        [0.9192, 0.1506, 0.1333],\n",
       "        [0.9009, 0.2255, 0.1081],\n",
       "        [0.8842, 0.2517, 0.1637],\n",
       "        [0.5747, 0.2227, 0.1921],\n",
       "        [0.3461, 0.3427, 0.1909],\n",
       "        [0.8736, 0.1625, 0.1305]], grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(a4, k=3, dim=-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import matplotlib.pyplot as plt\n",
    "\n",
    "backward_model = BackwardModel()\n",
    "s1, s2, s3 = backward_model(target)\n",
    "\n",
    "i1, i2 = 7, 2\n",
    "X = s1.detach().cpu()\n",
    "X = X / X.norm()\n",
    "X = X.numpy()\n",
    "print(X[i1,i2].min(), X[i1,i2].max())\n",
    "plt.imshow(X[i1,i2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i1, i2 = 0, 6\n",
    "X = a1.detach().cpu()\n",
    "X = X / X.norm()\n",
    "X = X.numpy()\n",
    "print(X[i1,i2].min(), X[i1,i2].max())\n",
    "plt.imshow(X[i1,i2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
