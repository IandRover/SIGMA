{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffa0c9f-8276-4e73-b6a8-bf33d01039d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"dcf9600e0485401cbb0ddbb0f7be1c70f96b32ef\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "import wandb\n",
    "from sigma_layer import SigmaLinear, SigmaConv, SigmaView\n",
    "from utils import get_dataset, gradient_centralization, normalize_along_axis, get_activation_function, compute_SCL_loss\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f367862f-05b5-4c98-b3be-7a36a686df88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: CIFAR10_SIGMA_conv32-actelu_SGD_1_2023\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "No LN1\n",
      "Epoch 0 | train loss 17.9213 | val loss 2.2800 | val acc 12.1200\n",
      "Epoch 1 | train loss 2.3914 | val loss 2.2908 | val acc 14.5700\n",
      "Epoch 2 | train loss 2.2945 | val loss 1.7685 | val acc 41.7100\n",
      "Epoch 3 | train loss 1.9678 | val loss 1.7497 | val acc 43.9300\n",
      "Epoch 4 | train loss 1.8506 | val loss 1.5259 | val acc 45.1800\n",
      "Epoch 5 | train loss 1.7675 | val loss 1.4396 | val acc 50.2400\n",
      "Epoch 6 | train loss 1.6690 | val loss 1.3974 | val acc 50.8500\n",
      "Epoch 7 | train loss 1.6375 | val loss 1.6040 | val acc 45.9300\n",
      "Epoch 8 | train loss 1.6379 | val loss 1.5328 | val acc 46.1200\n",
      "Epoch 9 | train loss 1.6205 | val loss 1.3357 | val acc 53.3300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 179\u001b[0m\n\u001b[1;32m    177\u001b[0m val_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSIGMA\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     activations \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mto(device), detach_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    180\u001b[0m     _, loss_item \u001b[39m=\u001b[39m criteria(activations, signals, target\u001b[39m.\u001b[39mto(device), method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlocal\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    182\u001b[0m \u001b[39melif\u001b[39;00m args\u001b[39m.\u001b[39mmethod \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBP\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m, in \u001b[0;36mSigmaModel_SimpleCNN.forward\u001b[0;34m(self, x, detach_grad, return_activations)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, detach_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_activations\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):   \n\u001b[1;32m     92\u001b[0m     \u001b[39m# x = self.LN1(x)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     a1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x, detach_grad)\n\u001b[1;32m     94\u001b[0m     a2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(a1, detach_grad)\n\u001b[1;32m     95\u001b[0m     a2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mview1(a2, detach_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/GitHub/SIGMA/sigma_layer.py:50\u001b[0m, in \u001b[0;36mSigmaConv.forward\u001b[0;34m(self, x, detach_grad)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, detach_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m detach_grad: x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m---> 50\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_act(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_layer(x))\n\u001b[1;32m     51\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_pool(x)\n\u001b[1;32m     52\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conv_forward(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "# Training scheme group\n",
    "method_parser = parser.add_argument_group(\"Method\")\n",
    "method_parser.add_argument('--method', type=str, default='SIGMA', choices=['SIGMA', 'BP', 'FA'])\n",
    "method_parser.add_argument('--architecture', type=str, default='lenet', choices=['lenet', 'vgg'])\n",
    "method_parser.add_argument('--actfunc', type=str, default='elu', choices=['tanh', 'elu', 'relu'])\n",
    "method_parser.add_argument('--conv_dim', type=int, default=32, choices=[32, 64])\n",
    "# Dataset group\n",
    "dataset_parser = parser.add_argument_group('Dataset')\n",
    "dataset_parser.add_argument('--dataset', type=str, default='CIFAR10', choices=['MNIST', 'CIFAR10'])\n",
    "dataset_parser.add_argument('--batchsize', type=int, default=64)\n",
    "dataset_parser.add_argument('--splitratio', type=float, default=0.2)\n",
    "# Training group # LR, optimizer, weight_decay, momentum\n",
    "training_parser = parser.add_argument_group('Training')\n",
    "training_parser.add_argument('--epochs', type=int, default=100)\n",
    "training_parser.add_argument('--lr', type=float, default=1)\n",
    "training_parser.add_argument('--optimizer', type=str, default='SGD', choices=['RMSprop', 'Adam', 'SGD'])\n",
    "# Seed group\n",
    "seed_parser = parser.add_argument_group('Seed')\n",
    "seed_parser.add_argument('--seed', type=int, default=2023)\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "# Set run_name\n",
    "run_name = f\"{args.dataset}_{args.method}_conv{args.conv_dim}-act{args.actfunc}_{args.optimizer}_{args.lr}_{args.seed}\"\n",
    "time_stamp = datetime.datetime.now().strftime(\"%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Set wandb\n",
    "wandb.init(\n",
    "    project=\"opt-sigma\",\n",
    "    name=run_name,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"algorithm\": args.method,\n",
    "    \"architecture\": \"SimpleCNN\",\n",
    "    \"dataset\": args.dataset,\n",
    "    \"epochs\": args.epochs,\n",
    "    \"lr\": args.lr,\n",
    "    \"optimizer\": args.optimizer,\n",
    "    \"seed\": args.seed,\n",
    "    \"conv_dim\": args.conv_dim,\n",
    "    \"actfunc\": args.actfunc,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Run name: {run_name}\")\n",
    "\n",
    "# Set seed\n",
    "torch.manual_seed(args.seed), np.random.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n",
    "\n",
    "# Get dataset\n",
    "train_loader, val_loader, test_loader = get_dataset(args)\n",
    "\n",
    "class SigmaModel_SimpleCNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SigmaModel_SimpleCNN, self).__init__()\n",
    "        if args.dataset == \"CIFAR10\":\n",
    "            self.LN1 = torch.nn.LayerNorm((3,32,32), elementwise_affine=False) \n",
    "            self.conv1 = SigmaConv(args, 3, 32, 5, 3, 2)\n",
    "            self.conv2 = SigmaConv(args, 32, 64, 3, 3, 2)\n",
    "            self.view1 = SigmaView((64, 8, 8), 64 * 8 * 8)\n",
    "            self.fc1 = SigmaLinear(args, 64 * 8 * 8, 128)\n",
    "            self.fc2 = SigmaLinear(args, 128, 10)\n",
    "            \n",
    "        elif args.dataset == \"MNIST\":\n",
    "            if args.architecture == \"lenet\":\n",
    "                self.LN1 = torch.nn.LayerNorm((1,28,28), elementwise_affine=False) \n",
    "                self.conv1 = SigmaConv(args, 1, 32, 5, 2, 3)\n",
    "                self.conv2 = SigmaConv(args, 32, 64, 5, 2, 3)\n",
    "                self.view1 = SigmaView((64, 4, 4), 64 * 4 * 4)\n",
    "                self.fc1 = SigmaLinear(args, 64 * 4 * 4, 512)\n",
    "                self.fc2 = SigmaLinear(args, 512, 10)\n",
    "\n",
    "        # orthoginal init self.fc1 and self.fc2\n",
    "        init.orthogonal_(self.fc1.backward_layer.weight)\n",
    "        init.sparse_(self.fc2.backward_layer.weight, sparsity=0.5)\n",
    "        # init.orthogonal_(self.fc2.backward_layer.weight)\n",
    "        # init.zeros_(self.fc1.forward_layer.weight)\n",
    "        # init.zeros_(self.fc2.forward_layer.weight)\n",
    "        # init.zeros_(self.conv2.forward_layer.weight)\n",
    "                \n",
    "        self.forward_params = list()\n",
    "        self.backward_params = list()\n",
    "        for layer in [self.conv1, self.conv2, self.fc1, self.fc2]:\n",
    "            forward_params, backward_params = layer.get_parameters()\n",
    "            self.forward_params += forward_params\n",
    "            self.backward_params += backward_params\n",
    "\n",
    "    def forward(self, x, detach_grad=False, return_activations=True):   \n",
    "        # x = self.LN1(x)\n",
    "        a1 = self.conv1(x, detach_grad)\n",
    "        a2 = self.conv2(a1, detach_grad)\n",
    "        a2 = self.view1(a2, detach_grad)\n",
    "        a3 = self.fc1(a2, detach_grad)\n",
    "        a4 = self.fc2(a3, detach_grad)\n",
    "        return [a1, a2, a3, a4]\n",
    "        \n",
    "    def reverse(self, target, detach_grad=True, return_activations=True):\n",
    "        if target.shape == torch.Size([10]): \n",
    "            target = F.one_hot(target, num_classes=10).float().to(target.device)\n",
    "        b3 = self.fc2.reverse(target, detach_grad)\n",
    "        b2 = self.fc1.reverse(b3, detach_grad)\n",
    "        b2 = self.view1.reverse(b2, detach_grad)\n",
    "        b1 = self.conv2.reverse(b2, detach_grad)\n",
    "        return [b1, b2, b3, target]\n",
    "\n",
    "\n",
    "class SigmaLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(SigmaLoss, self).__init__()\n",
    "        self.args = args\n",
    "        self.final_criteria = nn.CrossEntropyLoss()\n",
    "        self.local_criteria = compute_SCL_loss\n",
    "        self.method = args.method\n",
    "        \n",
    "    def forward(self, activations, signals, target, method=\"final\"):\n",
    "        if method == \"local\":\n",
    "            loss = list()\n",
    "            for act, sig in zip(activations[:-1], signals[:-1]):\n",
    "                loss += [self.local_criteria(act, sig, target)]\n",
    "            loss += [self.final_criteria(activations[-1], target)]\n",
    "            return sum(loss), loss[-1].item()\n",
    "        elif method == \"final\":\n",
    "            loss = self.final_criteria(activations[-1], target)\n",
    "            return loss, loss.item()\n",
    "        \n",
    "model = SigmaModel_SimpleCNN(args)\n",
    "model.to(device)\n",
    "if args.optimizer == \"SGD\": \n",
    "    forward_optimizer = optim.SGD(model.forward_params, lr=args.lr, weight_decay=0.0005)\n",
    "    backward_optimizer = optim.SGD(model.backward_params, lr=args.lr*20)\n",
    "\n",
    "elif args.optimizer == \"RMSprop\": forward_optimizer = optim.RMSprop(model.forward_params, lr=args.lr, weight_decay=0)\n",
    "elif args.optimizer == \"Adam\": forward_optimizer = optim.Adam(model.forward_params, lr=args.lr)\n",
    "criteria = SigmaLoss(args)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    signals = model.reverse(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long().to(device), return_activations=True)\n",
    "    \n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"No LN1\")\n",
    "# for epoch in tqdm(range(args.epochs)):\n",
    "for epoch in range(args.epochs):\n",
    "    \n",
    "    train_loss, train_counter = 0, 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > 100: continue\n",
    "        if args.method == \"SIGMA\":\n",
    "            activations = model(data.to(device), detach_grad=True)\n",
    "            signals = model.reverse(torch.Tensor([0,1,2,3,4,5,6,7,8,9]).long().to(device), detach_grad=True)\n",
    "            loss, loss_item = criteria(activations, signals, target.to(device), method=\"local\")\n",
    "        elif args.method == \"BP\":\n",
    "            activations = model(data.to(device), detach_grad=False)\n",
    "            loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        forward_optimizer.zero_grad(), backward_optimizer.zero_grad(), loss.backward()\n",
    "        gradient_centralization(model)\n",
    "        forward_optimizer.step(), backward_optimizer.step()\n",
    "        train_loss += loss_item * len(data)\n",
    "        train_counter += len(data)\n",
    "\n",
    "    wandb.log({'train_loss': train_loss / train_counter}, step=epoch)\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if batch_idx > 100: continue\n",
    "        activations = model(data.to(device), detach_grad=True)\n",
    "        loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        forward_optimizer.zero_grad(), loss.backward(), forward_optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    val_correct, val_loss, val_counter = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            val_counter += len(data)\n",
    "            if args.method == \"SIGMA\":\n",
    "                activations = model(data.to(device), detach_grad=True)\n",
    "                _, loss_item = criteria(activations, signals, target.to(device), method=\"local\")\n",
    "                \n",
    "            elif args.method == \"BP\":\n",
    "                activations = model(data.to(device), detach_grad=True)\n",
    "                _, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "            prediction = activations[-1].detach()\n",
    "            _, predicted = torch.max(prediction, 1)\n",
    "            val_correct += (predicted == target.to(device)).sum().item()\n",
    "            val_loss += loss_item * len(data)\n",
    "\n",
    "    wandb.log({'val_loss': val_loss / val_counter, \n",
    "               'val_acc': val_correct / val_counter, \n",
    "               }, step=epoch)\n",
    "    \n",
    "    print(f\"\"\"Epoch {epoch} | train loss {train_loss / train_counter:.4f} | val loss {val_loss / val_counter:.4f} | val acc {100 * val_correct / val_counter:.4f}\"\"\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save(best_model.state_dict(), f'./saved_models/{run_name}-{time_stamp}.pt')\n",
    "        \n",
    "# Eval on Test Set by loading the best model \n",
    "model.load_state_dict(torch.load(f'./saved_models/{run_name}-{time_stamp}.pt'))\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "test_loss, test_counter = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        outputs = model(data.to(device), detach_grad=False)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == target.to(device)).sum().item()\n",
    "        loss, loss_item = criteria(activations, signals, target.to(device), method=\"final\")\n",
    "        test_loss += loss_item * len(data)\n",
    "        test_counter += len(data)\n",
    "\n",
    "wandb.log({'test_loss': test_loss / test_counter,})\n",
    "print(f'Epoch: {epoch}, Test Accuracy: {100 * correct / test_counter:.4f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf785afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
